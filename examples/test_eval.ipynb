{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06206aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from breastfeeding_nlp.extraction.ner import MedSpaCyLabeler\n",
    "from breastfeeding_nlp.utils.utils import filter_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d4f47",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f416c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataframe that has the raw notes\n",
    "orig_df = pd.read_excel(\"/Volumes/RISIDataServices_MPrint_NCH/MPRINT_LACTATE_BF_1_20250424.xlsx\")\n",
    "orig_df.reset_index(names=['row_ix'], inplace=True)\n",
    "\n",
    "# Add the cohort splits\n",
    "cohort_splits = pd.read_csv(\"/Volumes/RISIDataServices_MPrint_NCH/data/cohort-split-unstratified.csv\")\n",
    "orig_df = orig_df.merge(cohort_splits, on=\"PAT_ID\")\n",
    "\n",
    "# drop 2 missing notes\n",
    "orig_df.dropna(subset='NOTE_TEXT', inplace=True)\n",
    "\n",
    "# drop 30 wic records\n",
    "wic_ids = orig_df.query(\"NOTE_TEXT.str.contains('Ohio WIC Prescribed Formula and Food Request Form')\").row_ix\n",
    "orig_df = orig_df.query(\"row_ix not in @wic_ids.tolist()\")\n",
    "\n",
    "# drop 3k note types\n",
    "filter_out_note_types = [\"Patient Instructions\", \"Discharge Instructions\", \"MR AVS Snapshot\", \"ED AVS Snapshot\", \"IP AVS Snapshot\", \"Training\", \"Operative Report\", \"D/C Planning\", \"Pharmacy\"]\n",
    "orig_df = orig_df[~orig_df[\"NOTE_TYPE\"].isin(filter_out_note_types)]\n",
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb415abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = orig_df.query(\"split == 'test'\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e11b3",
   "metadata": {},
   "source": [
    "# Load the medspaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0633573",
   "metadata": {},
   "outputs": [],
   "source": [
    "medspacy_labeler = MedSpaCyLabeler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5800b",
   "metadata": {},
   "source": [
    "# Run it on everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c824523",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df = medspacy_labeler.process_dataframe(test_df)\n",
    "medspacy_doc_labels = medspacy_labeler.label_documents(entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c961e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medspacy_doc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4c661",
   "metadata": {},
   "source": [
    "# Load in the gold standard labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c84481",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/cxg042/Documents/git/ods-preglac/dev_cg/output_all_notes.jsonl.out\", 'r') as f:\n",
    "    results = f.readlines()\n",
    "\n",
    "def organize_batch_results(batch_results):\n",
    "    dfs = []\n",
    "    for res in tqdm(batch_results):\n",
    "        dfs.append(parse_batch_results(res))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def parse_batch_results(single_result: Dict[str, str]) -> pd.DataFrame:\n",
    "    res = json.loads(single_result)\n",
    "\n",
    "    record_id = res['recordId']\n",
    "\n",
    "    input_tokens = res['modelOutput']['usage']['input_tokens']\n",
    "    output_tokens = res['modelOutput']['usage']['output_tokens']\n",
    "\n",
    "    label = json.loads(res['modelOutput']['content'][0]['text'])['Label']\n",
    "    reasoning = json.loads(res['modelOutput']['content'][0]['text'])['Reasoning']\n",
    "\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            \"recordID\": record_id,\n",
    "            \"Label\": label,\n",
    "            \"Reasoning\": reasoning,\n",
    "            \"input_cost\": (input_tokens/1_000_000) * 3,\n",
    "            \"output_cost\": (output_tokens/1_000_000) * 15,\n",
    "        }\n",
    "    ])\n",
    "\n",
    "def standardize_label(label):\n",
    "    if label == \"Absent / Insufficient Evidence\":\n",
    "        return \"absent\"\n",
    "    else:\n",
    "        return label.lower()\n",
    "\n",
    "res = organize_batch_results(results)\n",
    "res.recordID = res.recordID.astype(int)\n",
    "res.recordID -=1\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e641e",
   "metadata": {},
   "source": [
    "## Merge the two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.merge(\n",
    "    test_df[[\n",
    "        \"PAT_ID\", \"BF1\", \"BF2\", \"NOTE_TYPE\", \"NOTE_TEXT\", \"row_ix\", \"split\"\n",
    "    ]],\n",
    "    res.drop([\"input_cost\", \"output_cost\"], axis=1),\n",
    "    left_index=True,\n",
    "    right_on=\"recordID\",\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "res_df.Label = res_df.Label.apply(standardize_label)\n",
    "res_df = res_df.drop(columns=[\"BF1\", \"BF2\", \"NOTE_TEXT\", \"PAT_ID\", \"split\", \"recordID\", \"Reasoning\"])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = res_df.merge(medspacy_doc_labels, on='row_ix', how='inner')\n",
    "eval_df.rename(columns={'medspacy_document_label': 'medspacy_label'}, inplace=True)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8339796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix\n",
    "conf_mat = pd.crosstab(eval_df.medspacy_label, eval_df.Label)\n",
    "conf_mat.index = list(map(lambda x: x.title(), conf_mat.index))\n",
    "conf_mat.columns = list(map(lambda x: x.title(), conf_mat.columns))\n",
    "\n",
    "# Set plot size and style\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create heatmap with blue color palette\n",
    "ax = sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    cbar=False\n",
    ")\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('LLM Label', fontsize=12)\n",
    "ax.set_ylabel('MedspaCy Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix: MedspaCy vs LLM labels', fontsize=14, pad=20, loc='left', x=-0.07)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance\n",
    "y_true = eval_df.Label\n",
    "y_pred = eval_df.medspacy_label\n",
    "\n",
    "# Generate the classification report as a dictionary\n",
    "report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "# Convert to DataFrame for better formatting\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Filter out aggregate rows (like accuracy, macro avg, etc.) if only class-wise metrics are needed\n",
    "class_wise_report = report_df.loc[~report_df.index.str.contains(\"avg|accuracy\")]\n",
    "\n",
    "performance = class_wise_report.drop(columns=['support']).loc[[\"positive\", \"negative\", \"absent\"]].apply(lambda x: round(x, 2))\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddc225",
   "metadata": {},
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"/Volumes/RISIDataServices_MPrint_NCH/data/results/test_set_document_results.csv\")\n",
    "entities_df.to_csv(\"/Volumes/RISIDataServices_MPrint_NCH/data/results/test_set_entity_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d89e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
